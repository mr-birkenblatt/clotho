from typing import Any

import torch
import torch._C as _C
from _typeshed import Incomplete
from torch._six import with_metaclass as with_metaclass


class FunctionCtx:
    to_save: Incomplete
    def save_for_backward(self, *tensors: torch.Tensor): ...
    saved_for_forward: Incomplete
    def save_for_forward(self, *tensors: torch.Tensor): ...
    dirty_tensors: Incomplete
    def mark_dirty(self, *args: torch.Tensor): ...
    def mark_shared_storage(self, *pairs) -> None: ...
    non_differentiable: Incomplete
    def mark_non_differentiable(self, *args: torch.Tensor): ...
    materialize_grads: Incomplete
    def set_materialize_grads(self, value: bool): ...

class _HookMixin: ...

class BackwardCFunction(_C._FunctionBase, FunctionCtx, _HookMixin):
    def apply(self, *args): ...
    def apply_jvp(self, *args): ...

class FunctionMeta(type):
    def __init__(cls, name, bases, attrs) -> None: ...

class Function:
    def __init__(self, *args, **kwargs) -> None: ...
    def __call__(self, *args, **kwargs) -> None: ...
    is_traceable: bool
    @staticmethod
    def forward(ctx: Any, *args: Any, **kwargs: Any) -> Any: ...
    @staticmethod
    def backward(ctx: Any, *grad_outputs: Any) -> Any: ...
    vjp: Incomplete
    @staticmethod
    def jvp(ctx: Any, *grad_inputs: Any) -> Any: ...

def once_differentiable(fn): ...
def traceable(fn_cls): ...

class InplaceFunction(Function):
    inplace: Incomplete
    def __init__(self, inplace: bool = ...) -> None: ...

class NestedIOFunction(Function):
    def backward(self, *gradients: Any) -> Any: ...
    __call__: Incomplete
    def forward(self, *args: Any) -> Any: ...
    to_save: Incomplete
    def save_for_backward(self, *args: Any) -> None: ...
    @property
    def saved_tensors(self): ...
    dirty_tensors: Incomplete
    def mark_dirty(self, *args: Any, **kwargs: Any) -> None: ...
    non_differentiable: Incomplete
    def mark_non_differentiable(self, *args: Any, **kwargs: Any) -> None: ...
    def forward_extended(self, *input: Any) -> None: ...
    def backward_extended(self, *grad_output: Any) -> None: ...
