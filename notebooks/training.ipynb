{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c1265ba-c76b-4424-b577-a322e6b512af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in a terminal run\n",
    "# > make run-redis NS=train\n",
    "# > make run-redis NS=test\n",
    "# to allow access to the train and test namespaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e21c530-5f85-4b18-b9ed-c2fbe591b52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62721599-23d3-405a-bbb2-88a6f0a17a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"..\")\n",
    "os.environ[\"USER_PATH\"] = \"/mnt/d/workspace/clotho/userdata/\"\n",
    "MODEL_OUTPUT_BASE = \"/mnt/d/workspace/clotho/notebooks\"\n",
    "MODEL_OUTPUT_CP = os.path.join(MODEL_OUTPUT_BASE, \"checkpoints\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82c6f1c7-607b-479d-b2f6-3483cb91a57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from misc.redis import set_redis_slow_mode\n",
    "from misc.util import highest_number\n",
    "from model.datagenerator import create_train_test\n",
    "from system.namespace.store import get_namespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d8c7e9a-fa89-435d-a0a2-0a492056581e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "is_cuda = torch.cuda.is_available()\n",
    "is_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a897205-0707-4928-860d-5e8d58cef31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_redis_slow_mode(\"never\")\n",
    "ns_test = get_namespace(\"test\")\n",
    "ns_train = get_namespace(\"train\")\n",
    "now = pd.Timestamp(\"2022-12-17\", tz=\"UTC\")\n",
    "train_plan = [\n",
    "    {\n",
    "        \"left\": {\"mode\": \"valid\", \"flip_pc\": 0.5},\n",
    "        \"right\": {\"mode\": \"valid\", \"flip_pc\": 0.0},\n",
    "        \"min_text_length\": 20,\n",
    "        \"skip_weak\": True,\n",
    "        \"skip_topics\": True,\n",
    "        \"flip_lr\": 0.5,\n",
    "        \"first_epoch\": 10,\n",
    "        \"last_epoch\": None,\n",
    "        \"weight\": 100,\n",
    "    },\n",
    "    {\n",
    "        \"left\": {\"mode\": \"valid\", \"flip_pc\": 0.5},\n",
    "        \"right\": {\"mode\": \"valid\", \"flip_pc\": 0.0},\n",
    "        \"min_text_length\": None,\n",
    "        \"skip_weak\": True,\n",
    "        \"skip_topics\": True,\n",
    "        \"flip_lr\": 0.5,\n",
    "        \"first_epoch\": 10,\n",
    "        \"last_epoch\": None,\n",
    "        \"weight\": 100,\n",
    "    },\n",
    "    {\n",
    "        \"left\": {\"mode\": \"random\", \"flip_pc\": 0.0},\n",
    "        \"right\": {\"mode\": \"path\", \"flip_pc\": 0.0},\n",
    "        \"min_text_length\": 20,\n",
    "        \"skip_weak\": True,\n",
    "        \"skip_topics\": True,\n",
    "        \"flip_lr\": 0.5,\n",
    "        \"first_epoch\": None,\n",
    "        \"last_epoch\": None,\n",
    "        \"weight\": 60,\n",
    "    },\n",
    "    {\n",
    "        \"left\": None,\n",
    "        \"right\": {\"mode\": \"path\", \"flip_pc\": 0.0},\n",
    "        \"min_text_length\": 20,\n",
    "        \"skip_weak\": True,\n",
    "        \"skip_topics\": True,\n",
    "        \"flip_lr\": 0.5,\n",
    "        \"first_epoch\": None,\n",
    "        \"last_epoch\": None,\n",
    "        \"weight\": 40,\n",
    "    },\n",
    "     {\n",
    "        \"left\": {\"mode\": \"random\", \"flip_pc\": 0.0},\n",
    "        \"right\": {\"mode\": \"path\", \"flip_pc\": 0.0},\n",
    "        \"min_text_length\": None,\n",
    "        \"skip_weak\": True,\n",
    "        \"skip_topics\": True,\n",
    "        \"flip_lr\": 0.5,\n",
    "        \"first_epoch\": 5,\n",
    "        \"last_epoch\": None,\n",
    "        \"weight\": 60,\n",
    "    },\n",
    "    {\n",
    "        \"left\": None,\n",
    "        \"right\": {\"mode\": \"path\", \"flip_pc\": 0.0},\n",
    "        \"min_text_length\": None,\n",
    "        \"skip_weak\": True,\n",
    "        \"skip_topics\": True,\n",
    "        \"flip_lr\": 0.5,\n",
    "        \"first_epoch\": 5,\n",
    "        \"last_epoch\": None,\n",
    "        \"weight\": 40,\n",
    "    },\n",
    "    {\n",
    "        \"left\": {\"mode\": \"random\", \"flip_pc\": 0.0},\n",
    "        \"right\": {\"mode\": \"valid\", \"flip_pc\": 0.0},\n",
    "        \"min_text_length\": None,\n",
    "        \"skip_weak\": True,\n",
    "        \"skip_topics\": True,\n",
    "        \"flip_lr\": 0.5,\n",
    "        \"first_epoch\": None,\n",
    "        \"last_epoch\": None,\n",
    "        \"weight\": 60,\n",
    "    },\n",
    "    {\n",
    "        \"left\": None,\n",
    "        \"right\": {\"mode\": \"valid\", \"flip_pc\": 0.0},\n",
    "        \"min_text_length\": None,\n",
    "        \"skip_weak\": True,\n",
    "        \"skip_topics\": True,\n",
    "        \"flip_lr\": 0.5,\n",
    "        \"first_epoch\": None,\n",
    "        \"last_epoch\": None,\n",
    "        \"weight\": 40,\n",
    "    },\n",
    "    {\n",
    "        \"left\": {\"mode\": \"valid\", \"flip_pc\": 0.0},\n",
    "        \"right\": {\"mode\": \"valid\", \"flip_pc\": 0.0},\n",
    "        \"min_text_length\": None,\n",
    "        \"skip_weak\": True,\n",
    "        \"skip_topics\": True,\n",
    "        \"flip_lr\": 0,\n",
    "        \"first_epoch\": 15,\n",
    "        \"last_epoch\": None,\n",
    "        \"weight\": 50,\n",
    "    }\n",
    "]\n",
    "eval_plan = [\n",
    "    {\n",
    "        \"left\": {\"mode\": \"random\", \"flip_pc\": 0.0},\n",
    "        \"right\": {\"mode\": \"valid\", \"flip_pc\": 0.0},\n",
    "        \"min_text_length\": 20,\n",
    "        \"skip_weak\": True,\n",
    "        \"skip_topics\": True,\n",
    "        \"flip_lr\": 0.5,\n",
    "        \"weight\": 60,\n",
    "    },\n",
    "    {\n",
    "        \"left\": None,\n",
    "        \"right\": {\"mode\": \"valid\", \"flip_pc\": 0.0},\n",
    "        \"min_text_length\": 20,\n",
    "        \"skip_weak\": True,\n",
    "        \"skip_topics\": True,\n",
    "        \"flip_lr\": 0.5,\n",
    "        \"weight\": 40,\n",
    "    },\n",
    "    {\n",
    "        \"left\": {\"mode\": \"random\", \"flip_pc\": 0.0},\n",
    "        \"right\": {\"mode\": \"valid\", \"flip_pc\": 0.0},\n",
    "        \"min_text_length\": None,\n",
    "        \"skip_weak\": True,\n",
    "        \"skip_topics\": True,\n",
    "        \"flip_lr\": 0.5,\n",
    "        \"weight\": 60,\n",
    "    },\n",
    "    {\n",
    "        \"left\": None,\n",
    "        \"right\": {\"mode\": \"valid\", \"flip_pc\": 0.0},\n",
    "        \"min_text_length\": None,\n",
    "        \"skip_weak\": True,\n",
    "        \"skip_topics\": True,\n",
    "        \"flip_lr\": 0.5,\n",
    "        \"weight\": 40,\n",
    "    },\n",
    "]\n",
    "ttgen = create_train_test(\n",
    "    train_ns=ns_train,\n",
    "    train_validation_ns=ns_train,\n",
    "    test_ns=ns_test,\n",
    "    test_validation_ns=ns_test,\n",
    "    train_learning_plan=train_plan,\n",
    "    train_val_learning_plan=eval_plan,\n",
    "    test_learning_plan=eval_plan,\n",
    "    test_val_learning_plan=eval_plan,\n",
    "    batch_size=4 if is_cuda else 8,\n",
    "    epoch_batches=5000 if is_cuda else 500,\n",
    "    train_val_size=10000 if is_cuda else 1000,\n",
    "    test_size=10000 if is_cuda else 1000,\n",
    "    test_val_size=10000 if is_cuda else 1000,\n",
    "    compute_batch_size=100 if is_cuda else 100,\n",
    "    now=now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d7acd4d-0d4a-481f-901b-3e524ee02c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from transformers import DistilBertTokenizer, DistilBertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "927dbcbe-6df3-49c9-8449-041b1ab7a168",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if is_cuda else torch.device(\"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "00819f53-a953-4b85-bcf8-6cba59a8bb67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal, TypedDict\n",
    "\n",
    "ProviderRole = Literal[\"child\", \"parent\"]\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "EMBED_SIZE = 768\n",
    "\n",
    "TokenizedInput = TypedDict('TokenizedInput', {\n",
    "    \"input_ids\": torch.Tensor,\n",
    "    \"attention_mask\": torch.Tensor,\n",
    "})\n",
    "\n",
    "\n",
    "def tokens(texts: list[str]) -> TokenizedInput:\n",
    "    res = tokenizer(texts.tolist(), return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    return {k: v.to(device) for k, v in res.items()}\n",
    "\n",
    "\n",
    "class Noise(nn.Module):\n",
    "    def __init__(self, std: float = 1.0, p: float = 0.5) -> None:\n",
    "        super().__init__()\n",
    "        self._std = std\n",
    "        self._p = p\n",
    "        self._dhold = nn.Parameter(torch.Tensor([0.0]), requires_grad=False)\n",
    "\n",
    "    def set_std(self, std: float) -> None:\n",
    "        self._std = std\n",
    "\n",
    "    def get_std(self) -> float:\n",
    "        return self._std\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        if not self.training:\n",
    "            return x\n",
    "        prob = torch.rand(size=x.shape, device=self._dhold.device) < self._p\n",
    "        gauss = torch.normal(\n",
    "            mean=0.0, std=self._std, size=x.shape, device=self._dhold.device)\n",
    "        return x + prob * gauss\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, version: int) -> None:\n",
    "        super().__init__()\n",
    "        self._bert_parent = DistilBertModel.from_pretrained(\n",
    "            \"distilbert-base-uncased\")\n",
    "        self._bert_child = DistilBertModel.from_pretrained(\n",
    "            \"distilbert-base-uncased\")\n",
    "        if version in (1, 3, 4, 6):\n",
    "            self._pdense: nn.Sequential | None = nn.Sequential(\n",
    "                nn.Linear(EMBED_SIZE, EMBED_SIZE),\n",
    "                nn.Dropout(p=0.2),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(EMBED_SIZE, EMBED_SIZE))\n",
    "            self._cdense: nn.Sequential | None = nn.Sequential(\n",
    "                nn.Linear(EMBED_SIZE, EMBED_SIZE),\n",
    "                nn.Dropout(p=0.2),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(EMBED_SIZE, EMBED_SIZE))\n",
    "        else:\n",
    "            self._pdense = None\n",
    "            self._cdense = None\n",
    "        if version < 4 or version > 5:\n",
    "            self._noise = None\n",
    "        else:\n",
    "            self._noise = Noise(std=1.0, p=0.2)\n",
    "        if version < 2 or version > 4:\n",
    "            self._cos = None\n",
    "        else:\n",
    "            self._cos = torch.nn.CosineSimilarity()\n",
    "        if version < 6:\n",
    "            self._agg = \"cls\"\n",
    "        else:\n",
    "            self._agg = \"mean\"\n",
    "        self._version = version\n",
    "\n",
    "    def set_epoch(self, epoch: int) -> None:\n",
    "        noise = self._noise\n",
    "        if noise is not None:\n",
    "            noise.set_std(1 / (1.2 ** epoch))\n",
    "\n",
    "    def get_version(self) -> int:\n",
    "        return self._version\n",
    "    \n",
    "    def get_agg(self, lhs: torch.Tensor) -> torch.Tensor:\n",
    "        if self._agg == \"cls\":\n",
    "            return lhs[:, 0]\n",
    "        if self._agg == \"mean\":\n",
    "            return torch.mean(lhs, dim=1)\n",
    "        raise ValueError(f\"unknown aggregation: {self._agg}\")\n",
    "\n",
    "    def get_parent_embed(\n",
    "            self,\n",
    "            input_ids: torch.Tensor,\n",
    "            attention_mask: torch.Tensor) -> torch.Tensor:\n",
    "        outputs_parent = self._bert_parent(\n",
    "            input_ids=input_ids, attention_mask=attention_mask)\n",
    "        out = self.get_agg(outputs_parent.last_hidden_state)\n",
    "        if self._pdense is not None:\n",
    "            out = self._pdense(out)\n",
    "        if self._noise is not None:\n",
    "            out = self._noise(out)\n",
    "        return out\n",
    "\n",
    "    def get_child_embed(\n",
    "            self,\n",
    "            input_ids: torch.Tensor,\n",
    "            attention_mask: torch.Tensor) -> torch.Tensor:\n",
    "        outputs_child = self._bert_child(\n",
    "            input_ids=input_ids, attention_mask=attention_mask)\n",
    "        out = self.get_agg(outputs_child.last_hidden_state)\n",
    "        if self._cdense is not None:\n",
    "            out = self._cdense(out)\n",
    "        if self._noise is not None:\n",
    "            out = self._noise(out)\n",
    "        return out\n",
    "\n",
    "    def forward(self, x: dict[ProviderRole, TokenizedInput]) -> torch.Tensor:\n",
    "        parent_cls = self.get_parent_embed(\n",
    "            input_ids=x[\"parent\"][\"input_ids\"],\n",
    "            attention_mask=x[\"parent\"][\"attention_mask\"])\n",
    "        child_cls = self.get_child_embed(\n",
    "            input_ids=x[\"child\"][\"input_ids\"],\n",
    "            attention_mask=x[\"child\"][\"attention_mask\"])\n",
    "        if self._cos is not None:\n",
    "            return self._cos(parent_cls, child_cls).reshape([-1, 1])\n",
    "        batch_size = parent_cls.shape[0]\n",
    "        return torch.bmm(\n",
    "            parent_cls.reshape([batch_size, 1, -1]),\n",
    "            child_cls.reshape([batch_size, -1, 1])).reshape([-1, 1])\n",
    "\n",
    "\n",
    "class TrainingHarness(nn.Module):\n",
    "    def __init__(self, model: Model) -> None:\n",
    "        super().__init__()\n",
    "        self._model = model\n",
    "        self._softmax = nn.Softmax(dim=1)\n",
    "        self._loss = nn.BCELoss()\n",
    "\n",
    "    def get_version(self) -> int:\n",
    "        return self._model.get_version()\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            left: TokenizedInput,\n",
    "            right: TokenizedInput,\n",
    "            labels: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        out_left = self._model(left)\n",
    "        out_right = self._model(right)\n",
    "        preds = self._softmax(torch.hstack((out_left, out_right)))\n",
    "        return preds, self._loss(preds, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7823be84-27df-48b9-934a-2db72679e611",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filename(\n",
    "        harness: TrainingHarness,\n",
    "        *,\n",
    "        is_final: bool,\n",
    "        ftype: str,\n",
    "        epoch: int | None,\n",
    "        ext: str = \".pkl\") -> tuple[str, str, str]:\n",
    "    folder = MODEL_OUTPUT_BASE if is_final else MODEL_OUTPUT_CP\n",
    "    postfix = \"_lg\" if is_cuda else \"\"\n",
    "    version_tag = f\"_v{harness.get_version()}\"\n",
    "    out_pre = f\"{ftype}{version_tag}{postfix}_\"\n",
    "    out_post = ext\n",
    "    return (\n",
    "        os.path.join(\n",
    "            folder,\n",
    "            f\"{out_pre}{'' if epoch is None else epoch}{out_post}\"),\n",
    "        out_pre,\n",
    "        out_post,\n",
    "        folder,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f048a983-9daf-4f04-b36e-5da77951bb3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.weight', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.weight', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, 0)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.optim import AdamW\n",
    "\n",
    "model = Model(version=6)\n",
    "model.to(device)\n",
    "harness = TrainingHarness(model)\n",
    "harness.to(device)\n",
    "\n",
    "FORCE_RESTART = False\n",
    "\n",
    "_, spre, spost, sfolder = get_filename(harness, is_final=False, ftype=\"harness\", epoch=None)\n",
    "mprev = highest_number(os.listdir(sfolder), prefix=spre, postfix=spost)\n",
    "if not FORCE_RESTART and mprev is not None:\n",
    "    prev_fname, prev_epoch = mprev\n",
    "    harness.load_state_dict(torch.load(os.path.join(folder, prev_fname), map_location=device))\n",
    "    epoch_offset = prev_epoch + 1\n",
    "else:\n",
    "    epoch_offset = 0\n",
    "\n",
    "optimizer = AdamW(harness.parameters(), lr=5e-5)\n",
    "mprev, epoch_offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6cde9030-931f-4f80-a118-d7f5ef2677d9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da41b2e495124dec958b98ece8aabcbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/20000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'write'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/clotho/lib/python3.10/site-packages/torch/serialization.py:423\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[0;32m--> 423\u001b[0m     \u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_protocol\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/clotho/lib/python3.10/site-packages/torch/serialization.py:637\u001b[0m, in \u001b[0;36m_save\u001b[0;34m(obj, zip_file, pickle_module, pickle_protocol)\u001b[0m\n\u001b[1;32m    636\u001b[0m data_value \u001b[38;5;241m=\u001b[39m data_buf\u001b[38;5;241m.\u001b[39mgetvalue()\n\u001b[0;32m--> 637\u001b[0m \u001b[43mzip_file\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_record\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata.pkl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata_value\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    639\u001b[0m \u001b[38;5;66;03m# Write each tensor to a file named tensor/the_tensor_key in the zip archive\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'write'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 53\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m first:\n\u001b[1;32m     50\u001b[0m             \u001b[38;5;66;03m# display(train_df)\u001b[39;00m\n\u001b[1;32m     51\u001b[0m             first \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m---> 53\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mharness\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_filename\u001b[49m\u001b[43m(\u001b[49m\u001b[43mharness\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_final\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mftype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mharness\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     56\u001b[0m harness\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[0;32m~/miniconda3/envs/clotho/lib/python3.10/site-packages/torch/serialization.py:422\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    419\u001b[0m _check_dill_version(pickle_module)\n\u001b[1;32m    421\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[0;32m--> 422\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[1;32m    423\u001b[0m         _save(obj, opened_zipfile, pickle_module, pickle_protocol)\n\u001b[1;32m    424\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/clotho/lib/python3.10/site-packages/torch/serialization.py:299\u001b[0m, in \u001b[0;36m_open_zipfile_writer_buffer.__exit__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 299\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfile_like\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_end_of_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    300\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer\u001b[38;5;241m.\u001b[39mflush()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'write'"
     ]
    }
   ],
   "source": [
    "from transformers import get_scheduler\n",
    "# from tqdm.notebook import tqdm\n",
    "from tqdm.auto import tqdm\n",
    "import evaluate\n",
    "import time\n",
    "\n",
    "def compute(df):\n",
    "    plefts = tokens(df[\"parent_left\"])\n",
    "    clefts = tokens(df[\"child_left\"])\n",
    "    prights = tokens(df[\"parent_right\"])\n",
    "    crights = tokens(df[\"child_right\"])\n",
    "    labels = torch.tensor([~df[\"correct_is_right\"], df[\"correct_is_right\"]], dtype=torch.float32).T.to(device)\n",
    "    return harness({\"parent\": plefts, \"child\": clefts}, {\"parent\": prights, \"child\": crights}, labels)\n",
    "\n",
    "num_epochs = max((120 if is_cuda else 10) - epoch_offset, 3)\n",
    "num_training_steps = num_epochs * ttgen.get_epoch_train_size()\n",
    "warmup = 10000 if is_cuda else 10\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=warmup,\n",
    "    num_training_steps=num_training_steps - warmup)\n",
    "ttgen.set_epoch(epoch_offset)\n",
    "\n",
    "for _ in range(num_epochs):\n",
    "    epoch = ttgen.get_epoch()\n",
    "    print(f\"epoch {epoch}\")\n",
    "    real_time = time.monotonic()\n",
    "    \n",
    "    model.train()\n",
    "    harness.train()\n",
    "    model.set_epoch(epoch)\n",
    "    metric_train = evaluate.load(\"accuracy\")\n",
    "    train_loss = []\n",
    "    first = True\n",
    "    with tqdm(desc=\"train\", total=ttgen.get_epoch_train_size()) as progress_bar:\n",
    "        for train_df in ttgen.train_dfs():\n",
    "            preds, loss = compute(train_df)\n",
    "            train_loss.append(loss.item())\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            progress_bar.update(train_df.shape[0])\n",
    "            \n",
    "            predictions = torch.argmax(preds, dim=-1)\n",
    "            metric_train.add_batch(predictions=predictions, references=train_df[\"correct_is_right\"].astype(int))\n",
    "            if first:\n",
    "                # display(train_df)\n",
    "                first = False\n",
    "\n",
    "    torch.save(harness.state_dict(), get_filename(harness, is_final=False, ftype=\"harness\", epoch=epoch)[0])\n",
    "            \n",
    "    model.eval()\n",
    "    harness.eval()\n",
    "    with torch.no_grad():\n",
    "        metric_val_train = evaluate.load(\"accuracy\")\n",
    "        train_val_loss = []\n",
    "        with tqdm(desc=\"train val\", total=ttgen.get_epoch_train_validation_size()) as progress_bar:\n",
    "            for train_validation_df in ttgen.train_validation_dfs():\n",
    "                preds, loss = compute(train_validation_df)\n",
    "                train_val_loss.append(loss.item())\n",
    "                predictions = torch.argmax(preds, dim=-1)\n",
    "                metric_val_train.add_batch(\n",
    "                    predictions=predictions, references=train_validation_df[\"correct_is_right\"].astype(int))\n",
    "                progress_bar.update(train_validation_df.shape[0])\n",
    "        \n",
    "        metric_test = evaluate.load(\"accuracy\")\n",
    "        test_loss = []\n",
    "        with tqdm(desc=\"test\", total=ttgen.get_epoch_test_size()) as progress_bar:\n",
    "            for test_df in ttgen.test_dfs():\n",
    "                preds, loss = compute(test_df)\n",
    "                test_loss.append(loss.item())\n",
    "                predictions = torch.argmax(preds, dim=-1)\n",
    "                metric_test.add_batch(\n",
    "                    predictions=predictions, references=test_df[\"correct_is_right\"].astype(int))\n",
    "                progress_bar.update(test_df.shape[0])\n",
    "        stats = {\n",
    "            \"epoch\": int(epoch),\n",
    "            \"train_acc\": float(metric_train.compute()),\n",
    "            \"train_loss\": float(np.mean(train_loss)),\n",
    "            \"train_val_acc\": float(metric_val_train.compute()),\n",
    "            \"train_val_loss\": float(np.mean(train_val_loss)),\n",
    "            \"test_acc\": float(metric_test.compute()),\n",
    "            \"test_loss\": float(np.mean(test_loss)),\n",
    "            \"time\": 0.0,\n",
    "            \"version\": harness.get_version(),\n",
    "        }\n",
    "    \n",
    "    print(f\"train: {stats['train_acc']} loss: {stats['train_loss']}\")\n",
    "    print(f\"train val: {stats['train_val_acc']} loss: {stats['train_val_loss']}\")\n",
    "    print(f\"test: {stats['test_acc']} loss: {stats['test_loss']}\")\n",
    "    ttgen.advance_epoch()\n",
    "    stats[\"time\"] = float((time.monotonic() - real_time) / 60.0)\n",
    "    print(f\"epoch time: {stats['time']:.2f}min\")\n",
    "    stats_fn = get_filename(harness, is_final=False, ftype=\"stats\", epoch=epoch, ext=\".json\")[0]\n",
    "    with open(stats_fn, mode=\"w\", encoding=\"utf-8\") as fout:\n",
    "        print(json.dumps(stats, indent=2, sort_keys=True), file=fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a36224-b91d-4f21-9279-fa7c2a0e862f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), get_filename(harness, is_final=True, ftype=\"model\", epoch=None)[0])\n",
    "torch.save(harness.state_dict(), get_filename(harness, is_final=True, ftype=\"harness\", epoch=None)[0])\n",
    "torch.save(optimizer.state_dict(), get_filename(harness, is_final=True, ftype=\"optimizer\", epoch=None)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff582b9-1447-425a-9b0f-36d9366e4ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ttgen.reset()\n",
    "model.eval()\n",
    "harness.eval()\n",
    "dfs = []\n",
    "with torch.no_grad():\n",
    "    metric_val_test = evaluate.load(\"accuracy\")\n",
    "    test_val_loss = []\n",
    "    with tqdm(desc=\"test val\", total=ttgen.get_epoch_test_validation_size()) as progress_bar:\n",
    "        for test_val_df in ttgen.test_validation_dfs():\n",
    "            preds, loss = compute(test_val_df)\n",
    "            test_val_loss.append(loss.item())\n",
    "            predictions = torch.argmax(preds, dim=-1)\n",
    "            metric_val_test.add_batch(\n",
    "                predictions=predictions, references=test_val_df[\"correct_is_right\"].astype(int))\n",
    "            cur_df = test_val_df.copy()\n",
    "            cur_df[\"logit_left\"] = preds[:, 0].cpu()\n",
    "            cur_df[\"logit_right\"] = preds[:, 1].cpu()\n",
    "            cur_df[\"preds\"] = predictions.cpu()\n",
    "            cur_df[\"truth\"] = test_val_df[\"correct_is_right\"].astype(int)\n",
    "            dfs.append(cur_df)\n",
    "            progress_bar.update(test_val_df.shape[0])\n",
    "print(f\"test val: {metric_val_test.compute()} loss: {np.mean(test_val_loss)}\")\n",
    "validation_df = pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12c3406-9ff3-4576-93c2-daf914f3f8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_df.to_csv(\n",
    "    get_filename(harness, is_final=True, ftype=\"validation\", epoch=None, ext=\".csv\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b76f390-5d4c-496c-9f47-4fcab527a4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_df[validation_df[\"preds\"] == validation_df[\"truth\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dcb0772-acf6-40c0-aad0-6d2adbfcdadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_df[validation_df[\"preds\"] != validation_df[\"truth\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211e7f88-ff0c-4bad-bb47-2224bfedbc8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ttgen.reset()\n",
    "model.eval()\n",
    "harness.eval()\n",
    "with torch.no_grad():\n",
    "    count = 0\n",
    "    for test_val_df in ttgen.test_validation_dfs():\n",
    "        plefts = tokens(test_val_df[\"parent_left\"])\n",
    "        clefts = tokens(test_val_df[\"child_left\"])\n",
    "        prights = tokens(test_val_df[\"parent_right\"])\n",
    "        crights = tokens(test_val_df[\"child_right\"])\n",
    "        display(model.get_child_embed(\n",
    "            clefts[\"input_ids\"],\n",
    "            clefts[\"attention_mask\"]).cpu().numpy())\n",
    "        display(model.get_child_embed(\n",
    "            crights[\"input_ids\"],\n",
    "            crights[\"attention_mask\"]).cpu().numpy())\n",
    "        count += 1\n",
    "        if count >= 5:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98fd1aa-9723-4da8-8fcb-2d9500542f79",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:clotho]",
   "language": "python",
   "name": "conda-env-clotho-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
