{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c1265ba-c76b-4424-b577-a322e6b512af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in a terminal run\n",
    "# > make run-redis NS=train\n",
    "# > make run-redis NS=test\n",
    "# to allow access to the train and test namespaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e21c530-5f85-4b18-b9ed-c2fbe591b52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62721599-23d3-405a-bbb2-88a6f0a17a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"..\")\n",
    "os.environ[\"USER_PATH\"] = \"../userdata\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82c6f1c7-607b-479d-b2f6-3483cb91a57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from misc.redis import set_redis_slow_mode\n",
    "from misc.util import highest_number\n",
    "from model.datagenerator import create_train_test\n",
    "from system.namespace.store import get_namespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d8c7e9a-fa89-435d-a0a2-0a492056581e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "is_cuda = torch.cuda.is_available()\n",
    "is_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a897205-0707-4928-860d-5e8d58cef31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_redis_slow_mode(\"never\")\n",
    "ns_test = get_namespace(\"test\")\n",
    "ns_train = get_namespace(\"train\")\n",
    "now = pd.Timestamp(\"2022-12-17\", tz=\"UTC\")\n",
    "ttgen = create_train_test(\n",
    "    train_ns=ns_train,\n",
    "    train_validation_ns=ns_train,\n",
    "    test_ns=ns_test,\n",
    "    test_validation_ns=ns_test,\n",
    "    batch_size=4 if is_cuda else 8,\n",
    "    epoch_batches=5000 if is_cuda else 500,\n",
    "    train_val_size=10000 if is_cuda else 1000,\n",
    "    test_size=10000 if is_cuda else 1000,\n",
    "    test_val_size=10000 if is_cuda else 1000,\n",
    "    compute_batch_size=100 if is_cuda else 100,\n",
    "    conversation_based=False,\n",
    "    now=now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d7acd4d-0d4a-481f-901b-3e524ee02c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from transformers import DistilBertTokenizer, DistilBertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "927dbcbe-6df3-49c9-8449-041b1ab7a168",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if is_cuda else torch.device(\"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "00819f53-a953-4b85-bcf8-6cba59a8bb67",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "def tokens(texts):\n",
    "    res = tokenizer(texts.tolist(), return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    return {k: v.to(device) for k, v in res.items()}\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self._bert_parent = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "        self._bert_child = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "        \n",
    "    def get_parent_embed(self, input_ids, attention_mask):\n",
    "        outputs_parent = self._bert_parent(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        return outputs_parent.last_hidden_state[:, 0]\n",
    "    \n",
    "    def get_child_embed(self, input_ids, attention_mask):\n",
    "        outputs_child = self._bert_child(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        return outputs_child.last_hidden_state[:, 0]\n",
    "        \n",
    "    def forward(self, x):\n",
    "        parent_cls = self.get_parent_embed(input_ids=x[\"parent\"][\"input_ids\"], attention_mask=x[\"parent\"][\"attention_mask\"])\n",
    "        child_cls = self.get_child_embed(input_ids=x[\"child\"][\"input_ids\"], attention_mask=x[\"child\"][\"attention_mask\"])\n",
    "        batch_size = parent_cls.shape[0]\n",
    "        return torch.bmm(parent_cls.reshape([batch_size, 1, -1]), child_cls.reshape([batch_size, -1, 1])).reshape([-1, 1])\n",
    "    \n",
    "class TrainingHarness(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self._model = model\n",
    "        self._softmax = nn.Softmax(dim=1)\n",
    "        self._loss = nn.BCELoss()\n",
    "        \n",
    "    def forward(self, left, right, labels):\n",
    "        out_left = self._model(left)\n",
    "        out_right = self._model(right)\n",
    "        preds = self._softmax(torch.hstack((out_left, out_right)))\n",
    "        return preds, self._loss(preds, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f048a983-9daf-4f04-b36e-5da77951bb3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(('harness_lg_16.pkl', 16), 17)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.optim import AdamW\n",
    "\n",
    "model = Model()\n",
    "model.to(device)\n",
    "harness = TrainingHarness(model)\n",
    "harness.to(device)\n",
    "\n",
    "folder = \"checkpoints\"\n",
    "postfix = \"_lg\" if is_cuda else \"\"\n",
    "mprev = highest_number(os.listdir(folder), prefix=f\"harness{postfix}_\", postfix=\".pkl\")\n",
    "if mprev is not None:\n",
    "    prev_fname, prev_epoch = mprev\n",
    "    harness.load_state_dict(torch.load(os.path.join(folder, prev_fname), map_location=device))\n",
    "    epoch_offset = prev_epoch + 1\n",
    "else:\n",
    "    epoch_offset = 0\n",
    "\n",
    "optimizer = AdamW(harness.parameters(), lr=5e-5)\n",
    "mprev, epoch_offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6cde9030-931f-4f80-a118-d7f5ef2677d9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 17\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bea21eac688041c3976981f7b5876ca7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/20000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69a9522a23bb49479b8a17c3ff5168a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train val:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbaa7a24ff9f4af39420feb36d1eb149",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: {'accuracy': 0.6988} loss: 0.5092316172894702\n",
      "train val: {'accuracy': 0.7208} loss: 0.47745958423968404\n",
      "test: {'accuracy': 0.6657} loss: 0.6314230513453484\n",
      "epoch 18\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79d1d57ca373446d96fb363a3280a6a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/20000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ce0c5a302e8417dbf6a448ea5f8a235",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train val:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "error in compute thread",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m~/workspace/joschi/clotho/notebooks/../model/datagenerator.py:466\u001b[0m, in \u001b[0;36mTrainTestGenerator._th_run_train.<locals>.run\u001b[0;34m()\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 466\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_th_compute_batch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    467\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_alive\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_buff\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    468\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n",
      "File \u001b[0;32m~/workspace/joschi/clotho/notebooks/../model/datagenerator.py:429\u001b[0m, in \u001b[0;36mTrainTestGenerator._th_compute_batch\u001b[0;34m(self, is_alive, data, buff)\u001b[0m\n\u001b[1;32m    428\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 429\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compute_batch_for\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuff\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/workspace/joschi/clotho/notebooks/../model/datagenerator.py:416\u001b[0m, in \u001b[0;36mTrainTestGenerator._compute_batch_for\u001b[0;34m(self, data, buff)\u001b[0m\n\u001b[1;32m    414\u001b[0m rando, valid, flip_lr, flip_pc \u001b[38;5;241m=\u001b[39m row\n\u001b[1;32m    415\u001b[0m buff\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 416\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compute_row\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrando\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflip_lr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflip_pc\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cond:\n",
      "File \u001b[0;32m~/workspace/joschi/clotho/notebooks/../model/datagenerator.py:388\u001b[0m, in \u001b[0;36mTrainTestGenerator._compute_row\u001b[0;34m(data, left, right, flip_lr, flip_pc)\u001b[0m\n\u001b[1;32m    386\u001b[0m sway_right \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(sigmoid(score_right \u001b[38;5;241m-\u001b[39m score_left))\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[0;32m--> 388\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparent_left\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mleft\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    389\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchild_left\u001b[39m\u001b[38;5;124m\"\u001b[39m: data\u001b[38;5;241m.\u001b[39mget_text(left\u001b[38;5;241m.\u001b[39mget_child()),\n\u001b[1;32m    390\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparent_right\u001b[39m\u001b[38;5;124m\"\u001b[39m: data\u001b[38;5;241m.\u001b[39mget_text(right\u001b[38;5;241m.\u001b[39mget_parent()),\n\u001b[1;32m    391\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchild_right\u001b[39m\u001b[38;5;124m\"\u001b[39m: data\u001b[38;5;241m.\u001b[39mget_text(right\u001b[38;5;241m.\u001b[39mget_child()),\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msway_left\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m-\u001b[39m sway_right,\n\u001b[1;32m    393\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msway_right\u001b[39m\u001b[38;5;124m\"\u001b[39m: sway_right,\n\u001b[1;32m    394\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcorrect_is_right\u001b[39m\u001b[38;5;124m\"\u001b[39m: score_right \u001b[38;5;241m>\u001b[39m score_left,\n\u001b[1;32m    395\u001b[0m }\n",
      "File \u001b[0;32m~/workspace/joschi/clotho/notebooks/../model/datagenerator.py:197\u001b[0m, in \u001b[0;36mDataGenerator.get_text\u001b[0;34m(self, mhash)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_text\u001b[39m(\u001b[38;5;28mself\u001b[39m, mhash: MHash) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m--> 197\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_msgs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmhash\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msingle_line_text()\n",
      "File \u001b[0;32m~/workspace/joschi/clotho/notebooks/../system/msgs/disk.py:78\u001b[0m, in \u001b[0;36mDiskStore.read_message\u001b[0;34m(self, message_hash)\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\n\u001b[0;32m---> 78\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m msg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_file(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_path(message_hash)):\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m msg\u001b[38;5;241m.\u001b[39mget_hash() \u001b[38;5;241m==\u001b[39m message_hash:\n",
      "File \u001b[0;32m~/workspace/joschi/clotho/notebooks/../system/msgs/disk.py:69\u001b[0m, in \u001b[0;36mDiskStore._load_file\u001b[0;34m(self, fname)\u001b[0m\n\u001b[1;32m     68\u001b[0m mhash \u001b[38;5;241m=\u001b[39m msg\u001b[38;5;241m.\u001b[39mget_hash()\n\u001b[0;32m---> 69\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cache\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmhash\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01myield\u001b[39;00m msg\n",
      "File \u001b[0;32m~/workspace/joschi/clotho/notebooks/../misc/lru.py:30\u001b[0m, in \u001b[0;36mLRU.set\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_times[key] \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic()\n\u001b[0;32m---> 30\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/workspace/joschi/clotho/notebooks/../misc/lru.py:42\u001b[0m, in \u001b[0;36mLRU.gc\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m to_remove \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msorted\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_times\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mitem\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mitem\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_soft_limit]\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m rm_item \u001b[38;5;129;01min\u001b[39;00m to_remove:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: dictionary changed size during iteration",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 55\u001b[0m\n\u001b[1;32m     53\u001b[0m train_val_loss \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tqdm(desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain val\u001b[39m\u001b[38;5;124m\"\u001b[39m, total\u001b[38;5;241m=\u001b[39mttgen\u001b[38;5;241m.\u001b[39mget_epoch_train_validation_size()) \u001b[38;5;28;01mas\u001b[39;00m progress_bar:\n\u001b[0;32m---> 55\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m train_validation_df \u001b[38;5;129;01min\u001b[39;00m ttgen\u001b[38;5;241m.\u001b[39mtrain_validation_dfs():\n\u001b[1;32m     56\u001b[0m         preds, loss \u001b[38;5;241m=\u001b[39m compute(train_validation_df)\n\u001b[1;32m     57\u001b[0m         train_val_loss\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n",
      "File \u001b[0;32m~/workspace/joschi/clotho/notebooks/../model/datagenerator.py:653\u001b[0m, in \u001b[0;36mTrainTestGenerator.train_validation_dfs\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    652\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_validation_dfs\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterable[pd\u001b[38;5;241m.\u001b[39mDataFrame]:\n\u001b[0;32m--> 653\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m (\n\u001b[1;32m    654\u001b[0m         pd\u001b[38;5;241m.\u001b[39mDataFrame(val, columns\u001b[38;5;241m=\u001b[39mCOLUMNS)\n\u001b[1;32m    655\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m val \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_validation_batches()\n\u001b[1;32m    656\u001b[0m     )\n",
      "File \u001b[0;32m~/workspace/joschi/clotho/notebooks/../model/datagenerator.py:653\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    652\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_validation_dfs\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterable[pd\u001b[38;5;241m.\u001b[39mDataFrame]:\n\u001b[0;32m--> 653\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m (\n\u001b[1;32m    654\u001b[0m         pd\u001b[38;5;241m.\u001b[39mDataFrame(val, columns\u001b[38;5;241m=\u001b[39mCOLUMNS)\n\u001b[1;32m    655\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m val \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_validation_batches()\n\u001b[1;32m    656\u001b[0m     )\n",
      "File \u001b[0;32m~/workspace/joschi/clotho/notebooks/../model/datagenerator.py:623\u001b[0m, in \u001b[0;36mTrainTestGenerator.train_validation_batches\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    621\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain validation batches already exhausted!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 623\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext_train_validation_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    624\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m res \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    625\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/workspace/joschi/clotho/notebooks/../model/datagenerator.py:579\u001b[0m, in \u001b[0;36mTrainTestGenerator.next_train_validation_batch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cur_train_validation_size \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m train_val_size:\n\u001b[1;32m    578\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 579\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_batch_for\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    580\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_th_run_train_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_validation_buff\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    581\u001b[0m new_cur_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cur_train_validation_size \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlen\u001b[39m(res)\n\u001b[1;32m    582\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m new_cur_size \u001b[38;5;241m>\u001b[39m train_val_size:\n",
      "File \u001b[0;32m~/workspace/joschi/clotho/notebooks/../model/datagenerator.py:442\u001b[0m, in \u001b[0;36mTrainTestGenerator._get_batch_for\u001b[0;34m(self, start_th, buff)\u001b[0m\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_size):\n\u001b[1;32m    441\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_rows():\n\u001b[0;32m--> 442\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_err\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    443\u001b[0m         start_th()\n\u001b[1;32m    444\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cond:\n",
      "File \u001b[0;32m~/workspace/joschi/clotho/notebooks/../model/datagenerator.py:452\u001b[0m, in \u001b[0;36mTrainTestGenerator._check_err\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_err\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    451\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_th_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 452\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror in compute thread\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mself\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_th_err\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: error in compute thread"
     ]
    }
   ],
   "source": [
    "from transformers import get_scheduler\n",
    "# from tqdm.notebook import tqdm\n",
    "from tqdm.auto import tqdm\n",
    "import evaluate\n",
    "\n",
    "def compute(df):\n",
    "    plefts = tokens(df[\"parent_left\"])\n",
    "    clefts = tokens(df[\"child_left\"])\n",
    "    prights = tokens(df[\"parent_right\"])\n",
    "    crights = tokens(df[\"child_right\"])\n",
    "    labels = torch.tensor([~df[\"correct_is_right\"], df[\"correct_is_right\"]], dtype=torch.float32).T.to(device)\n",
    "    return harness({\"parent\": plefts, \"child\": clefts}, {\"parent\": prights, \"child\": crights}, labels)\n",
    "\n",
    "num_epochs = max((50 if is_cuda else 10) - epoch_offset, 3)\n",
    "num_training_steps = num_epochs * ttgen.get_epoch_train_size()\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps)\n",
    "ttgen.set_epoch(epoch_offset)\n",
    "\n",
    "for _ in range(num_epochs):\n",
    "    epoch = ttgen.get_epoch()\n",
    "    print(f\"epoch {epoch}\")\n",
    "    \n",
    "    model.train()\n",
    "    harness.train()\n",
    "    metric_train = evaluate.load(\"accuracy\")\n",
    "    train_loss = []\n",
    "    with tqdm(desc=\"train\", total=ttgen.get_epoch_train_size()) as progress_bar:\n",
    "        for train_df in ttgen.train_dfs():\n",
    "            preds, loss = compute(train_df)\n",
    "            train_loss.append(loss.item())\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            progress_bar.update(train_df.shape[0])\n",
    "            \n",
    "            predictions = torch.argmax(preds, dim=-1)\n",
    "            metric_train.add_batch(predictions=predictions, references=train_df[\"correct_is_right\"].astype(int))\n",
    "\n",
    "    folder = \"checkpoints\"\n",
    "    postfix = \"_lg\" if is_cuda else \"\"\n",
    "    torch.save(harness.state_dict(), os.path.join(folder, f\"harness{postfix}_{epoch}.pkl\"))\n",
    "            \n",
    "    model.eval()\n",
    "    harness.eval()\n",
    "    with torch.no_grad():\n",
    "        metric_val_train = evaluate.load(\"accuracy\")\n",
    "        train_val_loss = []\n",
    "        with tqdm(desc=\"train val\", total=ttgen.get_epoch_train_validation_size()) as progress_bar:\n",
    "            for train_validation_df in ttgen.train_validation_dfs():\n",
    "                preds, loss = compute(train_validation_df)\n",
    "                train_val_loss.append(loss.item())\n",
    "                predictions = torch.argmax(preds, dim=-1)\n",
    "                metric_val_train.add_batch(predictions=predictions, references=train_validation_df[\"correct_is_right\"].astype(int))\n",
    "                progress_bar.update(train_validation_df.shape[0])\n",
    "        \n",
    "        metric_test = evaluate.load(\"accuracy\")\n",
    "        test_loss = []\n",
    "        with tqdm(desc=\"test\", total=ttgen.get_epoch_test_size()) as progress_bar:\n",
    "            for test_df in ttgen.test_dfs():\n",
    "                preds, loss = compute(test_df)\n",
    "                test_loss.append(loss.item())\n",
    "                predictions = torch.argmax(preds, dim=-1)\n",
    "                metric_test.add_batch(predictions=predictions, references=test_df[\"correct_is_right\"].astype(int))\n",
    "                progress_bar.update(test_df.shape[0])\n",
    "        \n",
    "        print(f\"train: {metric_train.compute()} loss: {np.mean(train_loss)}\")\n",
    "        print(f\"train val: {metric_val_train.compute()} loss: {np.mean(train_val_loss)}\")\n",
    "        print(f\"test: {metric_test.compute()} loss: {np.mean(test_loss)}\")\n",
    "    ttgen.advance_epoch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a36224-b91d-4f21-9279-fa7c2a0e862f",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \".\"\n",
    "postfix = \"_lg\" if is_cuda else \"\"\n",
    "torch.save(model.state_dict(), os.path.join(folder, f\"model{postfix}.pkl\"))\n",
    "torch.save(harness.state_dict(), os.path.join(folder, f\"harness{postfix}.pkl\"))\n",
    "torch.save(optimizer.state_dict(), os.path.join(folder, f\"optimizer{postfix}.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff582b9-1447-425a-9b0f-36d9366e4ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ttgen.reset()\n",
    "model.eval()\n",
    "harness.eval()\n",
    "with torch.no_grad():\n",
    "    metric_val_test = evaluate.load(\"accuracy\")\n",
    "    test_val_loss = []\n",
    "    with tqdm(desc=\"test val\", total=ttgen.get_epoch_test_validation_size()) as progress_bar:\n",
    "        for test_val_df in ttgen.test_validation_dfs():\n",
    "            preds, loss = compute(test_val_df)\n",
    "            test_val_loss.append(loss.item())\n",
    "            predictions = torch.argmax(preds, dim=-1)\n",
    "            metric_val_test.add_batch(predictions=predictions, references=test_val_df[\"correct_is_right\"].astype(int))\n",
    "            progress_bar.update(test_val_df.shape[0])\n",
    "print(f\"test val: {metric_val_test.compute()} loss: {np.mean(test_val_loss)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:clotho]",
   "language": "python",
   "name": "conda-env-clotho-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
