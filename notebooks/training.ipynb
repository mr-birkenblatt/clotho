{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c1265ba-c76b-4424-b577-a322e6b512af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in a terminal run\n",
    "# > USER_PATH=/home/krause/userdata/ make run-redis NS=train\n",
    "# > USER_PATH=/home/krause/userdata/ make run-redis NS=test\n",
    "# to allow access to the train and test namespaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e21c530-5f85-4b18-b9ed-c2fbe591b52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Literal, TypedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62721599-23d3-405a-bbb2-88a6f0a17a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"..\")\n",
    "os.environ[\"USER_PATH\"] = \"/home/krause/userdata/\"\n",
    "MODEL_OUTPUT_BASE = \"/mnt/d/workspace/clotho/notebooks\"\n",
    "MODEL_OUTPUT_CP = os.path.join(MODEL_OUTPUT_BASE, \"checkpoints\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82c6f1c7-607b-479d-b2f6-3483cb91a57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from misc.redis import set_redis_slow_mode\n",
    "from misc.util import highest_number\n",
    "from misc.io import open_write\n",
    "from model.datagenerator import create_train_test\n",
    "from model.transformer_embed import (\n",
    "    get_epoch_and_load,\n",
    "    limit_epoch_data,\n",
    "    limit_epoch_data,\n",
    "    get_model_filename,\n",
    ")\n",
    "from system.namespace.store import get_namespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d8c7e9a-fa89-435d-a0a2-0a492056581e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "is_cuda = torch.cuda.is_available()\n",
    "is_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a897205-0707-4928-860d-5e8d58cef31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_redis_slow_mode(\"never\")\n",
    "ns_test = get_namespace(\"test\")\n",
    "ns_train = get_namespace(\"train\")\n",
    "now = pd.Timestamp(\"2022-12-17\", tz=\"UTC\")\n",
    "train_plan = [\n",
    "    {\n",
    "        \"left\": {\"mode\": \"valid\", \"flip_pc\": 1.0},\n",
    "        \"right\": {\"mode\": \"valid\", \"flip_pc\": 0.0},\n",
    "        \"min_text_length\": None,\n",
    "        \"skip_weak\": False,\n",
    "        \"skip_topics\": True,\n",
    "        \"flip_lr\": 0.5,\n",
    "        \"first_epoch\": 10,\n",
    "        \"last_epoch\": None,\n",
    "        \"weight\": 50,\n",
    "    },\n",
    "    {\n",
    "        \"left\": {\"mode\": \"valid\", \"flip_pc\": 1.0},\n",
    "        \"right\": {\"mode\": \"valid\", \"flip_pc\": 0.0},\n",
    "        \"min_text_length\": None,\n",
    "        \"skip_weak\": False,\n",
    "        \"skip_topics\": True,\n",
    "        \"flip_lr\": 0.5,\n",
    "        \"first_epoch\": 10,\n",
    "        \"last_epoch\": None,\n",
    "        \"weight\": 50,\n",
    "    },\n",
    "    {\n",
    "        \"left\": {\"mode\": \"random\", \"flip_pc\": 0.0},\n",
    "        \"right\": {\"mode\": \"path\", \"flip_pc\": 0.0},\n",
    "        \"min_text_length\": None,\n",
    "        \"skip_weak\": True,\n",
    "        \"skip_topics\": True,\n",
    "        \"flip_lr\": 0.5,\n",
    "        \"first_epoch\": None,\n",
    "        \"last_epoch\": None,\n",
    "        \"weight\": 60,\n",
    "    },\n",
    "    {\n",
    "        \"left\": None,\n",
    "        \"right\": {\"mode\": \"path\", \"flip_pc\": 0.0},\n",
    "        \"min_text_length\": None,\n",
    "        \"skip_weak\": True,\n",
    "        \"skip_topics\": True,\n",
    "        \"flip_lr\": 0.5,\n",
    "        \"first_epoch\": None,\n",
    "        \"last_epoch\": None,\n",
    "        \"weight\": 40,\n",
    "    },\n",
    "     {\n",
    "        \"left\": {\"mode\": \"random\", \"flip_pc\": 0.0},\n",
    "        \"right\": {\"mode\": \"path\", \"flip_pc\": 0.0},\n",
    "        \"min_text_length\": None,\n",
    "        \"skip_weak\": False,\n",
    "        \"skip_topics\": True,\n",
    "        \"flip_lr\": 0.5,\n",
    "        \"first_epoch\": 5,\n",
    "        \"last_epoch\": None,\n",
    "        \"weight\": 60,\n",
    "    },\n",
    "    {\n",
    "        \"left\": None,\n",
    "        \"right\": {\"mode\": \"path\", \"flip_pc\": 0.0},\n",
    "        \"min_text_length\": None,\n",
    "        \"skip_weak\": True,\n",
    "        \"skip_topics\": True,\n",
    "        \"flip_lr\": 0.5,\n",
    "        \"first_epoch\": 5,\n",
    "        \"last_epoch\": None,\n",
    "        \"weight\": 40,\n",
    "    },\n",
    "    {\n",
    "        \"left\": {\"mode\": \"random\", \"flip_pc\": 0.0},\n",
    "        \"right\": {\"mode\": \"valid\", \"flip_pc\": 0.0},\n",
    "        \"min_text_length\": None,\n",
    "        \"skip_weak\": True,\n",
    "        \"skip_topics\": True,\n",
    "        \"flip_lr\": 0.5,\n",
    "        \"first_epoch\": None,\n",
    "        \"last_epoch\": None,\n",
    "        \"weight\": 60,\n",
    "    },\n",
    "    {\n",
    "        \"left\": None,\n",
    "        \"right\": {\"mode\": \"valid\", \"flip_pc\": 0.0},\n",
    "        \"min_text_length\": None,\n",
    "        \"skip_weak\": True,\n",
    "        \"skip_topics\": True,\n",
    "        \"flip_lr\": 0.5,\n",
    "        \"first_epoch\": None,\n",
    "        \"last_epoch\": None,\n",
    "        \"weight\": 40,\n",
    "    },\n",
    "    {\n",
    "        \"left\": {\"mode\": \"valid\", \"flip_pc\": 1.0},\n",
    "        \"right\": {\"mode\": \"valid\", \"flip_pc\": 0.0},\n",
    "        \"min_text_length\": None,\n",
    "        \"skip_weak\": False,\n",
    "        \"skip_topics\": True,\n",
    "        \"flip_lr\": 0.5,\n",
    "        \"first_epoch\": 15,\n",
    "        \"last_epoch\": None,\n",
    "        \"weight\": 50,\n",
    "    }\n",
    "]\n",
    "eval_plan = [\n",
    "    {\n",
    "        \"left\": {\"mode\": \"random\", \"flip_pc\": 0.0},\n",
    "        \"right\": {\"mode\": \"valid\", \"flip_pc\": 0.0},\n",
    "        \"min_text_length\": None,\n",
    "        \"skip_weak\": False,\n",
    "        \"skip_topics\": True,\n",
    "        \"flip_lr\": 0.5,\n",
    "        \"weight\": 60,\n",
    "    },\n",
    "    {\n",
    "        \"left\": None,\n",
    "        \"right\": {\"mode\": \"valid\", \"flip_pc\": 0.0},\n",
    "        \"min_text_length\": None,\n",
    "        \"skip_weak\": False,\n",
    "        \"skip_topics\": True,\n",
    "        \"flip_lr\": 0.5,\n",
    "        \"weight\": 40,\n",
    "    },\n",
    "    {\n",
    "        \"left\": {\"mode\": \"random\", \"flip_pc\": 0.0},\n",
    "        \"right\": {\"mode\": \"valid\", \"flip_pc\": 0.0},\n",
    "        \"min_text_length\": None,\n",
    "        \"skip_weak\": False,\n",
    "        \"skip_topics\": True,\n",
    "        \"flip_lr\": 0.5,\n",
    "        \"weight\": 60,\n",
    "    },\n",
    "    {\n",
    "        \"left\": None,\n",
    "        \"right\": {\"mode\": \"valid\", \"flip_pc\": 0.0},\n",
    "        \"min_text_length\": None,\n",
    "        \"skip_weak\": False,\n",
    "        \"skip_topics\": True,\n",
    "        \"flip_lr\": 0.5,\n",
    "        \"weight\": 40,\n",
    "    },\n",
    "]\n",
    "ttgen = create_train_test(\n",
    "    train_ns=ns_train,\n",
    "    train_validation_ns=ns_train,\n",
    "    test_ns=ns_test,\n",
    "    test_validation_ns=ns_test,\n",
    "    train_learning_plan=train_plan,\n",
    "    train_val_learning_plan=eval_plan,\n",
    "    test_learning_plan=eval_plan,\n",
    "    test_val_learning_plan=eval_plan,\n",
    "    batch_size=4 if is_cuda else 8,\n",
    "    epoch_batches=5000 if is_cuda else 500,\n",
    "    train_val_size=10000 if is_cuda else 1000,\n",
    "    test_size=10000 if is_cuda else 1000,\n",
    "    test_val_size=10000 if is_cuda else 1000,\n",
    "    compute_batch_size=100 if is_cuda else 100,\n",
    "    now=now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d7acd4d-0d4a-481f-901b-3e524ee02c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "from transformers import DistilBertTokenizer, DistilBertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "927dbcbe-6df3-49c9-8449-041b1ab7a168",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if is_cuda else torch.device(\"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "00819f53-a953-4b85-bcf8-6cba59a8bb67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.transformer_embed import (\n",
    "    EMBED_SIZE,\n",
    "    TokenizedInput,\n",
    "    Model,\n",
    "    BaselineModel,\n",
    "    EitherModel,\n",
    "    TrainingHarness,\n",
    "    get_tokenizer,\n",
    ")\n",
    "\n",
    "\n",
    "tokens = get_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6cde9030-931f-4f80-a118-d7f5ef2677d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import get_scheduler\n",
    "# from tqdm.notebook import tqdm\n",
    "from tqdm.auto import tqdm\n",
    "import evaluate\n",
    "import time\n",
    "\n",
    "\n",
    "def create_model(version: int) -> EitherModel:\n",
    "    return Model(version) if version >= 0 else BaselineModel(version)\n",
    "\n",
    "\n",
    "def compute(harness, df):\n",
    "    plefts = tokens(df[\"parent_left\"].tolist())\n",
    "    clefts = tokens(df[\"child_left\"].tolist())\n",
    "    prights = tokens(df[\"parent_right\"].tolist())\n",
    "    crights = tokens(df[\"child_right\"].tolist())\n",
    "    labels = torch.tensor(\n",
    "        [~df[\"correct_is_right\"], df[\"correct_is_right\"]],\n",
    "        dtype=torch.float32).T.to(device)\n",
    "   \n",
    "    preds, loss = harness(\n",
    "        left={\"parent\": plefts, \"child\": clefts},\n",
    "        right={\"parent\": prights, \"child\": crights},\n",
    "        labels=labels)\n",
    "    # TODO add selective push losses\n",
    "    return preds, loss\n",
    "\n",
    "\n",
    "def run_training(num_epochs, version, force_restart):\n",
    "    model = create_model(version)\n",
    "    model.to(device)\n",
    "    harness = TrainingHarness(model)\n",
    "    harness.to(device)\n",
    "\n",
    "    mprev, epoch_offset = get_epoch_and_load(\n",
    "        harness,\n",
    "        MODEL_OUTPUT_CP,\n",
    "        ftype=\"harness\",\n",
    "        is_cuda=is_cuda,\n",
    "        device=device,\n",
    "        force_restart=force_restart)\n",
    "\n",
    "    optimizer = AdamW(harness.parameters(), lr=5e-5)\n",
    "    print(mprev, epoch_offset)\n",
    "    \n",
    "    num_epochs -= epoch_offset\n",
    "    if num_epochs <= 0:\n",
    "        print(\"already computed all epochs. nothing to do!\")\n",
    "        return model, harness, optimizer\n",
    "    \n",
    "    num_training_steps = num_epochs * ttgen.get_epoch_train_size()\n",
    "    warmup = 10000 if is_cuda else 10\n",
    "    lr_scheduler = get_scheduler(\n",
    "        name=\"linear\",\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=warmup,\n",
    "        num_training_steps=num_training_steps - warmup)\n",
    "    ttgen.set_epoch(epoch_offset)\n",
    "    \n",
    "    log_csv = get_model_filename(\n",
    "        harness,\n",
    "        MODEL_OUTPUT_BASE,\n",
    "        is_cuda=is_cuda,\n",
    "        ftype=\"val_log\",\n",
    "        epoch=None,\n",
    "        ext=\".csv\")\n",
    "    columns = [\n",
    "        \"epoch\",\n",
    "        \"train_acc\",\n",
    "        \"train_loss\",\n",
    "        \"train_val_acc\",\n",
    "        \"train_val_loss\",\n",
    "        \"test_acc\",\n",
    "        \"test_loss\",\n",
    "        \"time\",\n",
    "        \"version\",\n",
    "        \"fname\",\n",
    "    ]\n",
    "    if not os.path.exists(log_csv):\n",
    "        pd.DataFrame([], columns=columns).to_csv(\n",
    "            log_csv, header=True, mode=\"w\", columns=columns)\n",
    "\n",
    "    for _ in range(num_epochs):\n",
    "        epoch = ttgen.get_epoch()\n",
    "        print(f\"epoch {epoch} version: {harness.get_version()}\")\n",
    "        real_time = time.monotonic()\n",
    "\n",
    "        model.train()\n",
    "        harness.train()\n",
    "        model.set_epoch(epoch)\n",
    "        metric_train = evaluate.load(\"accuracy\")\n",
    "        train_loss = []\n",
    "        first = True\n",
    "        with tqdm(desc=\"train\", total=ttgen.get_epoch_train_size()) as progress_bar:\n",
    "            for train_df in ttgen.train_dfs():\n",
    "                preds, loss = compute(harness, train_df)\n",
    "                train_loss.append(loss.item())\n",
    "                loss.backward()\n",
    "\n",
    "                optimizer.step()\n",
    "                lr_scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "                progress_bar.update(train_df.shape[0])\n",
    "\n",
    "                predictions = torch.argmax(preds, dim=-1)\n",
    "                metric_train.add_batch(\n",
    "                    predictions=predictions,\n",
    "                    references=train_df[\"correct_is_right\"].astype(int))\n",
    "                if first:\n",
    "                    # display(train_df)\n",
    "                    first = False\n",
    "\n",
    "        model_fname = get_model_filename(\n",
    "            harness,\n",
    "            MODEL_OUTPUT_CP,\n",
    "            is_cuda=is_cuda,\n",
    "            ftype=\"harness\",\n",
    "            epoch=epoch)\n",
    "        torch.save(harness.state_dict(), model_fname)\n",
    "\n",
    "        model.eval()\n",
    "        harness.eval()\n",
    "        with torch.no_grad():\n",
    "            metric_val_train = evaluate.load(\"accuracy\")\n",
    "            train_val_loss = []\n",
    "            with tqdm(desc=\"train val\", total=ttgen.get_epoch_train_validation_size()) as progress_bar:\n",
    "                for train_validation_df in ttgen.train_validation_dfs():\n",
    "                    preds, loss = compute(harness, train_validation_df)\n",
    "                    train_val_loss.append(loss.item())\n",
    "                    predictions = torch.argmax(preds, dim=-1)\n",
    "                    metric_val_train.add_batch(\n",
    "                        predictions=predictions,\n",
    "                        references=train_validation_df[\"correct_is_right\"].astype(int))\n",
    "                    progress_bar.update(train_validation_df.shape[0])\n",
    "\n",
    "            metric_test = evaluate.load(\"accuracy\")\n",
    "            test_loss = []\n",
    "            with tqdm(desc=\"test\", total=ttgen.get_epoch_test_size()) as progress_bar:\n",
    "                for test_df in ttgen.test_dfs():\n",
    "                    preds, loss = compute(harness, test_df)\n",
    "                    test_loss.append(loss.item())\n",
    "                    predictions = torch.argmax(preds, dim=-1)\n",
    "                    metric_test.add_batch(\n",
    "                        predictions=predictions,\n",
    "                        references=test_df[\"correct_is_right\"].astype(int))\n",
    "                    progress_bar.update(test_df.shape[0])\n",
    "            stats = {\n",
    "                \"epoch\": int(epoch),\n",
    "                \"train_acc\": float(metric_train.compute()['accuracy']),\n",
    "                \"train_loss\": float(np.mean(train_loss)),\n",
    "                \"train_val_acc\": float(metric_val_train.compute()['accuracy']),\n",
    "                \"train_val_loss\": float(np.mean(train_val_loss)),\n",
    "                \"test_acc\": float(metric_test.compute()['accuracy']),\n",
    "                \"test_loss\": float(np.mean(test_loss)),\n",
    "                \"time\": 0.0,\n",
    "                \"version\": harness.get_version(),\n",
    "                \"fname\": model_fname,\n",
    "            }\n",
    "\n",
    "        print(f\"train: {stats['train_acc']} loss: {stats['train_loss']}\")\n",
    "        print(f\"train val: {stats['train_val_acc']} loss: {stats['train_val_loss']}\")\n",
    "        print(f\"test: {stats['test_acc']} loss: {stats['test_loss']}\")\n",
    "        ttgen.advance_epoch()\n",
    "        stats[\"time\"] = float((time.monotonic() - real_time) / 60.0)\n",
    "        print(f\"epoch time: {stats['time']:.2f}min\")\n",
    "        stats_fn = get_model_filename(\n",
    "            harness,\n",
    "            MODEL_OUTPUT_CP,\n",
    "            is_cuda=is_cuda,\n",
    "            ftype=\"stats\",\n",
    "            epoch=epoch,\n",
    "            ext=\".json\")\n",
    "        with open_write(stats_fn, text=True) as fout:\n",
    "            print(json.dumps(stats, indent=2, sort_keys=True), file=fout)\n",
    "        stats_df = pd.DataFrame(\n",
    "            {key: [val] for key, val in stats.items()},\n",
    "            columns=columns)\n",
    "        stats_df.to_csv(\n",
    "            log_csv, header=False, mode=\"a\")\n",
    "            \n",
    "        limit_epoch_data(\n",
    "            harness,\n",
    "            MODEL_OUTPUT_CP,\n",
    "            is_cuda=is_cuda,\n",
    "            ftype=\"stats\",\n",
    "            ext=\".json\",\n",
    "            count=5)\n",
    "    return model, harness, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c5a36224-b91d-4f21-9279-fa7c2a0e862f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, harness, optimizer):\n",
    "    torch.save(model.state_dict(), get_model_filename(\n",
    "        harness,\n",
    "        MODEL_OUTPUT_BASE,\n",
    "        is_cuda=is_cuda,\n",
    "        ftype=\"model\",\n",
    "        epoch=None))\n",
    "    torch.save(harness.state_dict(), get_model_filename(\n",
    "        harness,\n",
    "        MODEL_OUTPUT_BASE,\n",
    "        is_cuda=is_cuda,\n",
    "        ftype=\"harness\",\n",
    "        epoch=None))\n",
    "    torch.save(optimizer.state_dict(), get_model_filename(\n",
    "        harness,\n",
    "        MODEL_OUTPUT_BASE,\n",
    "        is_cuda=is_cuda,\n",
    "        ftype=\"optimizer\",\n",
    "        epoch=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bff582b9-1447-425a-9b0f-36d9366e4ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, harness):\n",
    "    ttgen.reset()\n",
    "    model.eval()\n",
    "    harness.eval()\n",
    "    dfs = []\n",
    "    with torch.no_grad():\n",
    "        metric_val_test = evaluate.load(\"accuracy\")\n",
    "        test_val_loss = []\n",
    "        with tqdm(desc=\"test val\", total=ttgen.get_epoch_test_validation_size()) as progress_bar:\n",
    "            for test_val_df in ttgen.test_validation_dfs():\n",
    "                preds, loss = compute(harness, test_val_df)\n",
    "                test_val_loss.append(loss.item())\n",
    "                predictions = torch.argmax(preds, dim=-1)\n",
    "                metric_val_test.add_batch(\n",
    "                    predictions=predictions,\n",
    "                    references=test_val_df[\"correct_is_right\"].astype(int))\n",
    "                cur_df = test_val_df.copy()\n",
    "                cur_df[\"logit_left\"] = preds[:, 0].cpu()\n",
    "                cur_df[\"logit_right\"] = preds[:, 1].cpu()\n",
    "                cur_df[\"preds\"] = predictions.cpu()\n",
    "                cur_df[\"truth\"] = test_val_df[\"correct_is_right\"].astype(int)\n",
    "                dfs.append(cur_df)\n",
    "                progress_bar.update(test_val_df.shape[0])\n",
    "    print(f\"test val: {metric_val_test.compute()} loss: {np.mean(test_val_loss)}\")\n",
    "    validation_df = pd.concat(dfs)\n",
    "    validation_df.to_csv(get_model_filename(\n",
    "        harness,\n",
    "        MODEL_OUTPUT_BASE,\n",
    "        is_cuda=is_cuda,\n",
    "        ftype=\"validation\",\n",
    "        epoch=None,\n",
    "        ext=\".csv\"))\n",
    "    print(\"correct\")\n",
    "    display(validation_df[validation_df[\"preds\"] == validation_df[\"truth\"]].head())\n",
    "    print(\"incorrect\")\n",
    "    display(validation_df[validation_df[\"preds\"] != validation_df[\"truth\"]].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "211e7f88-ff0c-4bad-bb47-2224bfedbc8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embeds(model):\n",
    "    ttgen.reset()\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        count = 0\n",
    "        for test_val_df in ttgen.test_validation_dfs():\n",
    "            plefts = tokens(test_val_df[\"parent_left\"].tolist())\n",
    "            clefts = tokens(test_val_df[\"child_left\"].tolist())\n",
    "            prights = tokens(test_val_df[\"parent_right\"].tolist())\n",
    "            crights = tokens(test_val_df[\"child_right\"].tolist())\n",
    "            display(model.get_child_embed(\n",
    "                clefts[\"input_ids\"],\n",
    "                clefts[\"attention_mask\"]).cpu().numpy())\n",
    "            display(model.get_child_embed(\n",
    "                crights[\"input_ids\"],\n",
    "                crights[\"attention_mask\"]).cpu().numpy())\n",
    "            count += 1\n",
    "            if count >= 5:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a98fd1aa-9723-4da8-8fcb-2d9500542f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_run(*, num_epochs, version, force_restart):\n",
    "    model, harness, optimizer = run_training(\n",
    "        num_epochs, version, force_restart)\n",
    "    save_model(model, harness, optimizer)\n",
    "    validation(model, harness)\n",
    "    embeds(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "28cae2dd-05dc-4864-80e2-64977d32ed3c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for version in range(8):\n",
    "#     full_run(num_epochs=30, version=version, force_restart=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f4a501c5-2870-4b82-b0d3-8df2df93a875",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None 0\n",
      "epoch 0 version: -1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31e40403156f438dac529e5ab18abab7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/20000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-27 14:19:35,633 INFO sqlalchemy.engine.Engine select pg_catalog.version()\n",
      "2023-01-27 14:19:35,634 INFO sqlalchemy.engine.Engine [raw sql] {}\n",
      "2023-01-27 14:19:35,635 INFO sqlalchemy.engine.Engine select current_schema()\n",
      "2023-01-27 14:19:35,636 INFO sqlalchemy.engine.Engine [raw sql] {}\n",
      "2023-01-27 14:19:35,637 INFO sqlalchemy.engine.Engine show standard_conforming_strings\n",
      "2023-01-27 14:19:35,638 INFO sqlalchemy.engine.Engine [raw sql] {}\n",
      "2023-01-27 14:19:35,640 INFO sqlalchemy.engine.Engine SELECT public.namespace.name, public.namespace.id \n",
      "FROM public.namespace\n",
      "2023-01-27 14:19:35,641 INFO sqlalchemy.engine.Engine [generated in 0.00067s] {}\n",
      "2023-01-27 14:19:35,647 INFO sqlalchemy.engine.Engine SELECT count(*) AS count_1 \n",
      "FROM public.msgs \n",
      "WHERE public.msgs.namespace_id = %(namespace_id_1)s\n",
      "2023-01-27 14:19:35,648 INFO sqlalchemy.engine.Engine [generated in 0.00039s] {'namespace_id_1': 1}\n",
      "2023-01-27 14:19:42,516 INFO sqlalchemy.engine.Engine SELECT anon_1.mhash, anon_1.row_id \n",
      "FROM (SELECT public.msgs.mhash AS mhash, row_number() OVER () AS row_id \n",
      "FROM public.msgs \n",
      "WHERE public.msgs.namespace_id = %(namespace_id_1)s) AS anon_1 \n",
      "WHERE row_id IN (%(row_id_1_1)s, %(row_id_1_2)s, %(row_id_1_3)s, %(row_id_1_4)s, %(row_id_1_5)s, %(row_id_1_6)s, %(row_id_1_7)s, %(row_id_1_8)s, %(row_id_1_9)s, %(row_id_1_10)s)\n",
      "2023-01-27 14:19:42,516 INFO sqlalchemy.engine.Engine [generated in 0.00061s] {'namespace_id_1': 1, 'row_id_1_1': 348374, 'row_id_1_2': 449700, 'row_id_1_3': 276663, 'row_id_1_4': 30179, 'row_id_1_5': 397893, 'row_id_1_6': 13808, 'row_id_1_7': 190673, 'row_id_1_8': 578559, 'row_id_1_9': 320218, 'row_id_1_10': 404781}\n",
      "2023-01-27 14:19:59,835 INFO sqlalchemy.engine.Engine SELECT count(*) AS count_1 \n",
      "FROM public.msgs \n",
      "WHERE public.msgs.namespace_id = %(namespace_id_1)s\n",
      "2023-01-27 14:19:59,836 INFO sqlalchemy.engine.Engine [cached since 24.19s ago] {'namespace_id_1': 1}\n",
      "2023-01-27 14:20:06,861 INFO sqlalchemy.engine.Engine SELECT anon_1.mhash, anon_1.row_id \n",
      "FROM (SELECT public.msgs.mhash AS mhash, row_number() OVER () AS row_id \n",
      "FROM public.msgs \n",
      "WHERE public.msgs.namespace_id = %(namespace_id_1)s) AS anon_1 \n",
      "WHERE row_id IN (%(row_id_1_1)s, %(row_id_1_2)s, %(row_id_1_3)s, %(row_id_1_4)s, %(row_id_1_5)s, %(row_id_1_6)s, %(row_id_1_7)s, %(row_id_1_8)s, %(row_id_1_9)s, %(row_id_1_10)s)\n",
      "2023-01-27 14:20:06,861 INFO sqlalchemy.engine.Engine [cached since 24.35s ago] {'namespace_id_1': 1, 'row_id_1_1': 670599, 'row_id_1_2': 527244, 'row_id_1_3': 672383, 'row_id_1_4': 9932, 'row_id_1_5': 330228, 'row_id_1_6': 110284, 'row_id_1_7': 621123, 'row_id_1_8': 118859, 'row_id_1_9': 294794, 'row_id_1_10': 188057}\n",
      "2023-01-27 14:20:21,959 INFO sqlalchemy.engine.Engine SELECT count(*) AS count_1 \n",
      "FROM public.msgs \n",
      "WHERE public.msgs.namespace_id = %(namespace_id_1)s\n",
      "2023-01-27 14:20:21,960 INFO sqlalchemy.engine.Engine [cached since 46.31s ago] {'namespace_id_1': 1}\n",
      "2023-01-27 14:20:29,177 INFO sqlalchemy.engine.Engine SELECT anon_1.mhash, anon_1.row_id \n",
      "FROM (SELECT public.msgs.mhash AS mhash, row_number() OVER () AS row_id \n",
      "FROM public.msgs \n",
      "WHERE public.msgs.namespace_id = %(namespace_id_1)s) AS anon_1 \n",
      "WHERE row_id IN (%(row_id_1_1)s, %(row_id_1_2)s, %(row_id_1_3)s, %(row_id_1_4)s, %(row_id_1_5)s, %(row_id_1_6)s, %(row_id_1_7)s, %(row_id_1_8)s, %(row_id_1_9)s, %(row_id_1_10)s)\n",
      "2023-01-27 14:20:29,178 INFO sqlalchemy.engine.Engine [cached since 46.66s ago] {'namespace_id_1': 1, 'row_id_1_1': 40197, 'row_id_1_2': 27579, 'row_id_1_3': 404037, 'row_id_1_4': 556309, 'row_id_1_5': 155897, 'row_id_1_6': 38854, 'row_id_1_7': 59732, 'row_id_1_8': 619134, 'row_id_1_9': 194247, 'row_id_1_10': 188937}\n",
      "2023-01-27 14:20:45,076 INFO sqlalchemy.engine.Engine SELECT count(*) AS count_1 \n",
      "FROM public.msgs \n",
      "WHERE public.msgs.namespace_id = %(namespace_id_1)s\n",
      "2023-01-27 14:20:45,076 INFO sqlalchemy.engine.Engine [cached since 69.43s ago] {'namespace_id_1': 1}\n",
      "2023-01-27 14:20:52,112 INFO sqlalchemy.engine.Engine SELECT anon_1.mhash, anon_1.row_id \n",
      "FROM (SELECT public.msgs.mhash AS mhash, row_number() OVER () AS row_id \n",
      "FROM public.msgs \n",
      "WHERE public.msgs.namespace_id = %(namespace_id_1)s) AS anon_1 \n",
      "WHERE row_id IN (%(row_id_1_1)s, %(row_id_1_2)s, %(row_id_1_3)s, %(row_id_1_4)s, %(row_id_1_5)s, %(row_id_1_6)s, %(row_id_1_7)s, %(row_id_1_8)s, %(row_id_1_9)s, %(row_id_1_10)s)\n",
      "2023-01-27 14:20:52,113 INFO sqlalchemy.engine.Engine [cached since 69.6s ago] {'namespace_id_1': 1, 'row_id_1_1': 199376, 'row_id_1_2': 524780, 'row_id_1_3': 556420, 'row_id_1_4': 394493, 'row_id_1_5': 412370, 'row_id_1_6': 496323, 'row_id_1_7': 197472, 'row_id_1_8': 247986, 'row_id_1_9': 264618, 'row_id_1_10': 160380}\n",
      "2023-01-27 14:21:08,676 INFO sqlalchemy.engine.Engine SELECT count(*) AS count_1 \n",
      "FROM public.msgs \n",
      "WHERE public.msgs.namespace_id = %(namespace_id_1)s\n",
      "2023-01-27 14:21:08,676 INFO sqlalchemy.engine.Engine [cached since 93.03s ago] {'namespace_id_1': 1}\n",
      "2023-01-27 14:21:15,709 INFO sqlalchemy.engine.Engine SELECT anon_1.mhash, anon_1.row_id \n",
      "FROM (SELECT public.msgs.mhash AS mhash, row_number() OVER () AS row_id \n",
      "FROM public.msgs \n",
      "WHERE public.msgs.namespace_id = %(namespace_id_1)s) AS anon_1 \n",
      "WHERE row_id IN (%(row_id_1_1)s, %(row_id_1_2)s, %(row_id_1_3)s, %(row_id_1_4)s, %(row_id_1_5)s, %(row_id_1_6)s, %(row_id_1_7)s, %(row_id_1_8)s, %(row_id_1_9)s, %(row_id_1_10)s)\n",
      "2023-01-27 14:21:15,709 INFO sqlalchemy.engine.Engine [cached since 93.19s ago] {'namespace_id_1': 1, 'row_id_1_1': 149978, 'row_id_1_2': 268626, 'row_id_1_3': 27138, 'row_id_1_4': 518514, 'row_id_1_5': 403013, 'row_id_1_6': 342567, 'row_id_1_7': 155130, 'row_id_1_8': 17845, 'row_id_1_9': 255504, 'row_id_1_10': 256978}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mfull_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mversion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_restart\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[14], line 2\u001b[0m, in \u001b[0;36mfull_run\u001b[0;34m(num_epochs, version, force_restart)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfull_run\u001b[39m(\u001b[38;5;241m*\u001b[39m, num_epochs, version, force_restart):\n\u001b[0;32m----> 2\u001b[0m     model, harness, optimizer \u001b[38;5;241m=\u001b[39m \u001b[43mrun_training\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mversion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_restart\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     save_model(model, harness, optimizer)\n\u001b[1;32m      5\u001b[0m     validation(model, harness)\n",
      "Cell \u001b[0;32mIn[10], line 95\u001b[0m, in \u001b[0;36mrun_training\u001b[0;34m(num_epochs, version, force_restart)\u001b[0m\n\u001b[1;32m     93\u001b[0m first \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tqdm(desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m, total\u001b[38;5;241m=\u001b[39mttgen\u001b[38;5;241m.\u001b[39mget_epoch_train_size()) \u001b[38;5;28;01mas\u001b[39;00m progress_bar:\n\u001b[0;32m---> 95\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m train_df \u001b[38;5;129;01min\u001b[39;00m ttgen\u001b[38;5;241m.\u001b[39mtrain_dfs():\n\u001b[1;32m     96\u001b[0m         preds, loss \u001b[38;5;241m=\u001b[39m compute(harness, train_df)\n\u001b[1;32m     97\u001b[0m         train_loss\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n",
      "File \u001b[0;32m~/workspace/joschi/clotho/notebooks/../model/datagenerator.py:901\u001b[0m, in \u001b[0;36mTrainTestGenerator.train_dfs\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    900\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_dfs\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterable[pd\u001b[38;5;241m.\u001b[39mDataFrame]:\n\u001b[0;32m--> 901\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m (\n\u001b[1;32m    902\u001b[0m         pd\u001b[38;5;241m.\u001b[39mDataFrame(val, columns\u001b[38;5;241m=\u001b[39mCOLUMNS)\n\u001b[1;32m    903\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m val \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_batches()\n\u001b[1;32m    904\u001b[0m     )\n",
      "File \u001b[0;32m~/workspace/joschi/clotho/notebooks/../model/datagenerator.py:901\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    900\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_dfs\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterable[pd\u001b[38;5;241m.\u001b[39mDataFrame]:\n\u001b[0;32m--> 901\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m (\n\u001b[1;32m    902\u001b[0m         pd\u001b[38;5;241m.\u001b[39mDataFrame(val, columns\u001b[38;5;241m=\u001b[39mCOLUMNS)\n\u001b[1;32m    903\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m val \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_batches()\n\u001b[1;32m    904\u001b[0m     )\n",
      "File \u001b[0;32m~/workspace/joschi/clotho/notebooks/../model/datagenerator.py:868\u001b[0m, in \u001b[0;36mTrainTestGenerator.train_batches\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    866\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain batches already exhausted!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    867\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 868\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext_train_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m res \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    870\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/workspace/joschi/clotho/notebooks/../model/datagenerator.py:816\u001b[0m, in \u001b[0;36mTrainTestGenerator.next_train_batch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    814\u001b[0m cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cur_train_cache\n\u001b[1;32m    815\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cur_train_ix \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(cache):\n\u001b[0;32m--> 816\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_batch_for\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_th_run_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_buff\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    817\u001b[0m end_ix \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(\n\u001b[1;32m    818\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cur_train_ix \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_size, train_size)\n\u001b[1;32m    819\u001b[0m res \u001b[38;5;241m=\u001b[39m cache[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cur_train_ix:end_ix]\n",
      "File \u001b[0;32m~/workspace/joschi/clotho/notebooks/../model/datagenerator.py:677\u001b[0m, in \u001b[0;36mTrainTestGenerator._get_batch_for\u001b[0;34m(self, start_th, buff, out)\u001b[0m\n\u001b[1;32m    675\u001b[0m         start_th()\n\u001b[1;32m    676\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cond:\n\u001b[0;32m--> 677\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait_for\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhas_rows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    678\u001b[0m     out\u001b[38;5;241m.\u001b[39mappend(buff\u001b[38;5;241m.\u001b[39mpopleft())\n\u001b[1;32m    679\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_err()\n",
      "File \u001b[0;32m~/miniconda3/envs/clotho/lib/python3.10/threading.py:355\u001b[0m, in \u001b[0;36mCondition.wait_for\u001b[0;34m(self, predicate, timeout)\u001b[0m\n\u001b[1;32m    353\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m waittime \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    354\u001b[0m                 \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m--> 355\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwaittime\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    356\u001b[0m     result \u001b[38;5;241m=\u001b[39m predicate()\n\u001b[1;32m    357\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/miniconda3/envs/clotho/lib/python3.10/threading.py:324\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    323\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 324\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    325\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    326\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "full_run(num_epochs=10, version=-1, force_restart=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6e5294-644a-4ae7-8eff-06e8ec593037",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "full_run(num_epochs=10, version=7, force_restart=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01bc6a02-fccc-4ebd-aa12-42f1887bc2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_run(num_epochs=10, version=0, force_restart=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb7b7fd-7e2d-4253-8192-f670106b8702",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_run(num_epochs=10, version=5, force_restart=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871b92df-f498-4a04-af33-b82e4c9a0688",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_run(num_epochs=30, version=-1, force_restart=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4993bd3-e5c1-4a92-a3b6-4c3e1c1c1ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_run(num_epochs=30, version=7, force_restart=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0abb55e-a0e7-4bc5-a17d-eb0ba4ce07f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_run(num_epochs=30, version=0, force_restart=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f0cfbf-e1c2-4ef2-877a-968f7911a956",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_run(num_epochs=30, version=5, force_restart=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f22222-b2bd-4af9-bd14-254d394a9575",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_run(num_epochs=60, version=-1, force_restart=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a274797a-104c-4aba-87e4-8ac860c7b6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_run(num_epochs=60, version=7, force_restart=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20325a9-6827-4eb0-988e-4c5d9a8791dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_run(num_epochs=60, version=0, force_restart=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08e677f-9171-4bb0-8ad2-11dfb550a56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_run(num_epochs=60, version=5, force_restart=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b973aae0-24c0-4866-aded-d815291112d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_run(num_epochs=90, version=-1, force_restart=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8129b19-aec9-474f-ab8c-9805c2989ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_run(num_epochs=90, version=7, force_restart=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1303bc8-be04-4280-a6b9-d96f0dcccb93",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_run(num_epochs=90, version=0, force_restart=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f99d8a-422f-42e5-b950-8ce08f3eacb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_run(num_epochs=90, version=5, force_restart=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a296d879-ace0-4c46-a0a5-e0e161a8742e",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_run(num_epochs=120, version=-1, force_restart=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15e9ab6-0327-4e6c-b308-0b2b9673b73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_run(num_epochs=120, version=7, force_restart=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e79c85-14f8-4302-8cf6-21be9e19562c",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_run(num_epochs=120, version=0, force_restart=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde8d78f-327b-43d6-abaa-96b3784c871c",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_run(num_epochs=120, version=5, force_restart=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f1fc11-2b71-4453-a6fa-3a73de48f9f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:clotho]",
   "language": "python",
   "name": "conda-env-clotho-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
